{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim-1.0.1-py3.5-win-amd64.egg\\gensim\\utils.py:855: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2017-06-18 23:58:05,107 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "logging.root.handlers = []  # Jupyter messes up logging so needs a reset\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "from smart_open import smart_open\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import random\n",
    "import gensim\n",
    "import nltk\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import linear_model\n",
    "from nltk.corpus import stopwords\n",
    "import xml.etree.ElementTree as ET\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pymorphy2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tree_df(tree_name):\n",
    "    tree = ET.parse(tree_name)\n",
    "    sentences = []\n",
    "    label = []\n",
    "    data_item = []\n",
    "    data = []\n",
    "    polarity=[]\n",
    "    root = tree.getroot()\n",
    "    for sentence in root.iter('sentence'):\n",
    "        for text in sentence.iter('text'):\n",
    "            for opinion in sentence.iter('Opinion'):\n",
    "                data_item.append(text.text)\n",
    "                data_item.append(opinion.get('category'))\n",
    "                data_item.append(opinion.get('polarity'))\n",
    "                data.append(data_item)\n",
    "                data_item=[]\n",
    "    df = pd.DataFrame(data, columns = ['sentence', 'category', 'polarity'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-18 23:58:13,253 : INFO : Loading dictionaries from C:\\Anaconda3\\lib\\site-packages\\pymorphy2_dicts\\data\n",
      "2017-06-18 23:58:14,181 : INFO : format: 2.4, revision: 393442, updated: 2015-01-17T16:03:56.586168\n"
     ]
    }
   ],
   "source": [
    "name = 'se16_ru_rest_train.xml'\n",
    "df = tree_df(name)\n",
    "test_name = 'se16_ru_rest_test.xml'\n",
    "df_test = tree_df(test_name)\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from stop_words import get_stop_words\n",
    "stop_words = get_stop_words('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_SVC(X_train, y_train, X_test, y_test):\n",
    "    clf = SVC(kernel='linear', random_state=241, probability=True)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_pred = pd.Series(y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1=metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "    return df_pred, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_logreg(X_train, y_train, X_test, y_test):\n",
    "    clf = linear_model.LogisticRegression(n_jobs=1, C=1e5)\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_pred = pd.Series(y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1=metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "    return df_pred, accuracy, precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_NB(X_train, y_train, X_test, y_test):\n",
    "    clf = MultinomialNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_pred = pd.Series(y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1=metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "    return df_pred, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_random_forest(X_train, y_train, X_test, y_test):\n",
    "    clf = RandomForestClassifier( n_estimators = 100, random_state=241 )\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_pred = pd.Series(y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'macro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'macro')\n",
    "    f1=metrics.f1_score(y_test, y_pred, average = 'macro')\n",
    "    return df_pred, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BAG OF WORDS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 169 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "count_vectorizer = CountVectorizer(stop_words=stop_words, max_features=1000,analyzer=\"word\")\n",
    "train_1 = count_vectorizer.fit_transform(df['sentence']).toarray()\n",
    "test_1 = count_vectorizer.transform(df_test['sentence']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlb = mlb = MultiLabelBinarizer()\n",
    "df_all = pd.concat([df, df_test], axis = 0)\n",
    "categories = mlb.fit_transform(df_all['category'])\n",
    "train_2 = categories[:len(df)]\n",
    "test_2 = categories[len(df):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train = df['polarity']\n",
    "y_test = df_test['polarity']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_bow = np.concatenate((test_1,test_2), axis=1)\n",
    "X_train_bow = np.concatenate((train_1,train_2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.69923, precision = 0.39138, recall = 0.34242, f1 = 0.35225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_svc, accuracy_bow_SVC, precision_bow_SVC, recall_bow_SVC, f1_bow_SVC = classify_SVC(X_train_bow,y_train,X_test_bow,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_bow_SVC, precision_bow_SVC, recall_bow_SVC, f1_bow_SVC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.61923, precision = 0.32338, recall = 0.32369, f1 = 0.32337\n"
     ]
    }
   ],
   "source": [
    "df_pred_logreg, accuracy_bow_logreg, precision_bow_logreg, recall_bow_logreg, f1_bow_logreg = classify_logreg(X_train_bow,y_train,X_test_bow,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_bow_logreg, precision_bow_logreg, recall_bow_logreg, f1_bow_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.71692, precision = 0.38504, recall = 0.33437, f1 = 0.33945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_NB, accuracy_bow_NB, precision_bow_NB, recall_bow_NB, f1_bow_NB = classify_NB(X_train_bow,y_train,X_test_bow,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_bow_NB, precision_bow_NB, recall_bow_NB, f1_bow_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.68769, precision = 0.36779, recall = 0.32085, f1 = 0.32589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_random_forest, accuracy_bow_random_forest, precision_bow_random_forest, recall_bow_random_forest, f1_bow_random_forest = classify_random_forest(X_train_bow,y_train,X_test_bow,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_bow_random_forest, precision_bow_random_forest, recall_bow_random_forest, f1_bow_random_forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# TF.IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tfidfv = TfidfVectorizer(stop_words=stop_words, max_features=1000,analyzer=\"word\")\n",
    "train_1_tfidf = tfidfv.fit_transform(df['sentence']).toarray()\n",
    "test_1_tfidf = tfidfv.transform(df_test['sentence']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test_tfidf = np.concatenate((test_1_tfidf,test_2), axis=1)\n",
    "X_train_tfidf = np.concatenate((train_1_tfidf,train_2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.70769, precision = 0.45891, recall = 0.32242, f1 = 0.32843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_tfidf_SVC, accuracy_tfidf_SVC, precision_tfidf_SVC, recall_tfidf_SVC, f1_tfidf_SVC = classify_SVC(X_train_tfidf,y_train,X_test_tfidf,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_tfidf_SVC, precision_tfidf_SVC, recall_tfidf_SVC, f1_tfidf_SVC))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.62000, precision = 0.32228, recall = 0.32121, f1 = 0.32141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_tfidf_logreg, accuracy_tfidf_logreg, precision_tfidf_logreg, recall_tfidf_logreg, f1_tfidf_logreg = classify_logreg(X_train_tfidf,y_train,X_test_tfidf,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_tfidf_logreg, precision_tfidf_logreg, recall_tfidf_logreg, f1_tfidf_logreg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.71462, precision = 0.35275, recall = 0.31266, f1 = 0.30750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_tfidf_NB, accuracy_tfidf_NB, precision_tfidf_NB, recall_tfidf_NB, f1_tfidf_NB = classify_NB(X_train_tfidf,y_train,X_test_tfidf,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_tfidf_NB, precision_tfidf_NB, recall_tfidf_NB, f1_tfidf_NB))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.69077, precision = 0.38021, recall = 0.31052, f1 = 0.31294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_tfidf_forest, accuracy_tfidf_forest, precision_tfidf_forest, recall_forest_NB, f1_tfidf_forest = classify_random_forest(X_train_tfidf,y_train,X_test_tfidf,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_tfidf_forest, precision_tfidf_forest, recall_forest_NB, f1_tfidf_forest))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sen_clear = []\n",
    "for sentence in np.array(df['sentence']):\n",
    "    sen_cl = re.sub(r'[^\\w\\d\\s\\-\\n\\{\\}]', \" \", sentence, flags=re.M | re.U | re.I)\n",
    "    #words = sen_cl.lower().split()\n",
    "    words = sen_cl.split()\n",
    "    words_c = []\n",
    "    for word in words:\n",
    "        if word != '-':\n",
    "            words_c.append(word)\n",
    "    words_stop = []\n",
    "    for word in words_c:\n",
    "        for stop_word in stop_words:\n",
    "            if word != stop_word:\n",
    "                words_stop.append(word)\n",
    "                break\n",
    "    sen_clear.append(words_stop)\n",
    "sen_clear_test = []\n",
    "for sentence in np.array(df_test['sentence']):\n",
    "    sen_cl = re.sub(r'[^\\w\\d\\s\\-\\n\\{\\}]', \" \", sentence, flags=re.M | re.U | re.I)\n",
    "    #words = sen_cl.lower().split()\n",
    "    words = sen_cl.split()\n",
    "    words_c = []\n",
    "    for word in words:\n",
    "        if word != '-':\n",
    "                words_c.append(word)\n",
    "    words_stop = []\n",
    "    for word in words_c:\n",
    "        for stop_word in stop_words:\n",
    "            if word != stop_word:\n",
    "                words_stop.append(word)\n",
    "                break\n",
    "    sen_clear_test.append(words_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2017-06-19 00:04:50,357 : INFO : collecting all words and their counts\n",
      "2017-06-19 00:04:50,371 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2017-06-19 00:04:50,403 : INFO : collected 7328 word types from a corpus of 60535 raw words and 4089 sentences\n",
      "2017-06-19 00:04:50,404 : INFO : Loading a fresh vocabulary\n",
      "2017-06-19 00:04:50,431 : INFO : min_count=5 retains 2127 unique words (29% of original 7328, drops 5201)\n",
      "2017-06-19 00:04:50,436 : INFO : min_count=5 leaves 51029 word corpus (84% of original 60535, drops 9506)\n",
      "2017-06-19 00:04:50,457 : INFO : deleting the raw counts dictionary of 7328 items\n",
      "2017-06-19 00:04:50,469 : INFO : sample=0.001 downsamples 51 most-common words\n",
      "2017-06-19 00:04:50,470 : INFO : downsampling leaves estimated 41857 word corpus (82.0% of prior 51029)\n",
      "2017-06-19 00:04:50,471 : INFO : estimated required memory for 2127 words and 1000 dimensions: 18079500 bytes\n",
      "2017-06-19 00:04:50,488 : INFO : resetting layer weights\n",
      "2017-06-19 00:04:50,638 : INFO : training model with 4 workers on 2127 vocabulary and 1000 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2017-06-19 00:04:50,640 : INFO : expecting 4089 sentences, matching count from corpus used for vocabulary survey\n",
      "2017-06-19 00:04:51,706 : INFO : PROGRESS: at 59.41% examples, 120385 words/s, in_qsize 8, out_qsize 0\n",
      "2017-06-19 00:04:52,161 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2017-06-19 00:04:52,202 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2017-06-19 00:04:52,236 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2017-06-19 00:04:52,237 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2017-06-19 00:04:52,238 : INFO : training on 302675 raw words (209194 effective words) took 1.6s, 133639 effective words/s\n",
      "2017-06-19 00:04:52,239 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.Word2Vec(sen_clear,size=1000,window=5,min_count=5, workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    # Function to average all of the word vectors in a given\n",
    "    # paragraph\n",
    "    #\n",
    "    # Pre-initialize an empty numpy array (for speed)\n",
    "    featureVec = np.zeros((num_features,))\n",
    "    #\n",
    "    nwords = 0.\n",
    "    # \n",
    "    # Index2word is a list that contains the names of the words in \n",
    "    # the model's vocabulary. Convert it to a set, for speed \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    #\n",
    "    # Loop over each word in the review and, if it is in the model's\n",
    "    # vocaublary, add its feature vector to the total\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    # \n",
    "    # Divide the result by the number of words to get the average\n",
    "    if nwords != 0.0:\n",
    "        featureVec = np.true_divide(featureVec,nwords)\n",
    "    else:\n",
    "        featureVec = np.zeros((num_features,))\n",
    "    return featureVec\n",
    "\n",
    "\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    # \n",
    "    # Initialize a counter\n",
    "    counter = 0.\n",
    "    # \n",
    "    # Preallocate a 2D numpy array, for speed\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features))\n",
    "    #reviewFeatureVecs = np.array((len(reviews),num_features))\n",
    "    # \n",
    "    # Loop through the reviews\n",
    "    for i in range(len(reviews)):\n",
    "        reviewFeatureVecs[i] = makeFeatureVec(reviews[i], model, num_features)\n",
    "        \n",
    "    #for review in reviews:\n",
    "\n",
    "       # Call the function (defined above) that makes average feature vectors\n",
    "       #reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "       #    num_features)\n",
    "       #\n",
    "       # Increment the counter\n",
    "       #counter = counter + 1.\n",
    "    \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for test reviews\n"
     ]
    }
   ],
   "source": [
    "# ****************************************************************\n",
    "# Calculate average feature vectors for training and testing sets,\n",
    "# using the functions we defined above. Notice that we now use stop word\n",
    "# removal.\n",
    "\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs( sen_clear, model, 1000 )\n",
    "\n",
    "print (\"Creating average feature vecs for test reviews\")\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs( sen_clear_test, model, 1000 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_test_w2v = np.concatenate((testDataVecs,test_2), axis=1)\n",
    "X_train_w2v = np.concatenate((trainDataVecs,train_2), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.66923, precision = 0.16731, recall = 0.25000, f1 = 0.20046\n",
      "accuracy = 0.72538, precision = 0.44596, recall = 0.33756, f1 = 0.34715\n",
      "accuracy = 0.68385, precision = 0.31461, recall = 0.27610, f1 = 0.25522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_w2v_SVC, accuracy_w2v_SVC, precision_w2v_SVC, recall_w2v_SVC, f1_w2v_SVC = classify_SVC(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_SVC, precision_w2v_SVC, recall_w2v_SVC, f1_w2v_SVC))\n",
    "\n",
    "\n",
    "df_pred_w2v_logreg, accuracy_w2v_logreg, precision_w2v_logreg, recall_w2v_logreg, f1_w2v_logreg = classify_logreg(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_logreg, precision_w2v_logreg, recall_w2v_logreg, f1_w2v_logreg))\n",
    "\n",
    "\n",
    "#df_pred_w2v_NB, accuracy_w2v_NB, precision_w2v_NB, recall_w2v_NB, f1_w2v_NB = classify_NB(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "#print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_NB, precision_w2v_NB, recall_w2v_NB, f1_w2v_NB))\n",
    "\n",
    "\n",
    "df_pred_w2v_forest, accuracy_w2v_forest, precision_w2v_forest, recall_w2v_forest, f1_w2v_forest = classify_random_forest(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_forest, precision_w2v_forest, recall_w2v_forest, f1_w2v_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.68385, precision = 0.31461, recall = 0.27610, f1 = 0.25522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_w2v_forest, accuracy_w2v_forest, precision_w2v_forest, recall_w2v_forest, f1_w2v_forest = classify_random_forest(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_forest, precision_w2v_forest, recall_w2v_forest, f1_w2v_forest))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "def classify_NB(X_train, y_train, X_test, y_test):\n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train,y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    df_pred = pd.Series(y_pred)\n",
    "    accuracy = metrics.accuracy_score(y_test, y_pred)\n",
    "    precision = metrics.precision_score(y_test, y_pred, average = 'micro')\n",
    "    recall = metrics.recall_score(y_test, y_pred, average = 'micro')\n",
    "    f1=metrics.f1_score(y_test, y_pred, average = 'micro')\n",
    "    return df_pred, accuracy, precision, recall, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy = 0.41308, precision = 0.25676, recall = 0.26899, f1 = 0.22037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "df_pred_w2v_NB, accuracy_w2v_NB, precision_w2v_NB, recall_w2v_NB, f1_w2v_NB = classify_NB(X_train_w2v,y_train,X_test_w2v,y_test)\n",
    "print('accuracy = {:0.5f}, precision = {:0.5f}, recall = {:0.5f}, f1 = {:0.5f}'.format(accuracy_w2v_NB, precision_w2v_NB, recall_w2v_NB, f1_w2v_NB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
